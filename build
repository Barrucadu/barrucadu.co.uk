#!/usr/bin/env python3

"""build

Usage:
  build [--out=<dir>] [--root=<url>]
  build (-h | --help)

Options:
  -h --help       Show this screen.
  --out=<dir>     Directory to generate site in [default: _site]
  --root=<url>    Root of the website [default: https://www.barrucadu.co.uk/]

"""

import hashlib
import jinja2
import shutil
import sys
import yaml

from bs4 import BeautifulSoup
from docopt import docopt
from pathlib import Path


def filter_akk(body, jinja2_env):
    def parse_akkline(body, defns):
        out = {}
        while body:
            line = body[0]
            if line.startswith("%akk:trans"):
                body = body[1:]
                out["trans"] = line.split(maxsplit=1)[1].strip()
            elif line.startswith("%akk:orig"):
                out["orig"] = line.split(maxsplit=1)[1].strip()
                body = body[1:]
            elif line.startswith("%akk:note"):
                is_html = line.startswith("%akk:note:html")
                rest = line.split(maxsplit=1)[1].strip()
                out.setdefault("notes", []).append((rest, is_html))
                body = body[1:]
            elif line.startswith("%akk:vocab"):
                body = body[1:]
                rest = line.split(maxsplit=1)[1]
                if ":" in rest:
                    term, defn = rest.split(":", maxsplit=1)
                    term = term.strip()
                    defn = defn.strip()
                    defns[term.strip()] = defn
                else:
                    term = rest.strip()
                out.setdefault("vocab", []).append((term, defns[term]))
            else:
                break
        return out, body

    lineno = 0
    defns = {}
    fragment_template = jinja2_env.get_template("_fragments/akkline.html")

    while body:
        if body[0].startswith("%akk:trans"):
            akkline, body = parse_akkline(body, defns)
            if akkline:
                lineno += 1
                yield fragment_template.render(
                    **akkline,
                    lineno=lineno,
                )
        else:
            yield body[0]
            body = body[1:]


FILTERS = {
    "akk": filter_akk,
}


args = docopt(__doc__)
OUT_DIR = args["--out"]
BASE_HREF = args["--root"]

JINJA2_ENV = jinja2.Environment(
    autoescape=jinja2.select_autoescape(["html", "xml"]),
    loader=jinja2.FileSystemLoader("_templates"),
)

shutil.rmtree(OUT_DIR, ignore_errors=True)
shutil.copytree("static", OUT_DIR, dirs_exist_ok=True)

HASHED_LINKS = {}
for fpath in Path("hashed-static").iterdir():
    if not fpath.is_file():
        continue

    file_hash = hashlib.sha256()
    with fpath.open(mode="rb") as f:
        while True:
            data = f.read(65536)
            if not data:
                break
            file_hash.update(data)

    HASHED_LINKS[fpath.name] = (
        f"{fpath.stem}-sha256-{file_hash.hexdigest()}{fpath.suffix}"
    )
    shutil.copy(fpath, Path(OUT_DIR, HASHED_LINKS[fpath.name]))


def render(template, target, params={}):
    if type(template) is str:
        template = JINJA2_ENV.get_template(template)
    if type(target) is str:
        target = Path(OUT_DIR, target)

    params["link"] = str(target).replace(f"{OUT_DIR}/", "")
    params["permalink"] = f"{BASE_HREF}{params['link']}"

    rendered = template.render(
        base_href=BASE_HREF,
        hashed_links=HASHED_LINKS,
        **params,
    )
    target.write_text(rendered)

    return rendered


def render_page(source, target):
    lines = source.read_text().splitlines()
    metadata_end_idx = lines[1:].index("---") + 1
    body_start_idx = metadata_end_idx + 1
    metadata = yaml.load("\n".join(lines[1:metadata_end_idx]), Loader=yaml.SafeLoader)
    body = lines[body_start_idx:]

    metadata.setdefault("template", "default.html")
    metadata.setdefault("filters", [])

    for name in metadata["filters"]:
        body = list(FILTERS[name](body, JINJA2_ENV))

    rendered = render(
        JINJA2_ENV.from_string(
            f"{{% extends \"{metadata['template']}\" %}}\n" + "\n".join(body),
        ),
        target,
        metadata,
    )

    soup = BeautifulSoup(rendered, features="html.parser")
    links = []
    for tag, attr in [("a", "href"), ("img", "src"), ("link", "href")]:
        for elem in soup.find_all(tag):
            link = elem.attrs[attr]
            anchor = None
            if "#" in link:
                link, anchor = link.split("#")
            links.append((link, anchor))

    return metadata, set(links)


PERMALINKS = []
links_to_check = {}
for root, dirs, files in Path("content").walk():
    target_dir = Path(OUT_DIR, *root.parts[1:])
    for dir in dirs:
        Path(target_dir, dir).mkdir(exist_ok=True)
    for file in files:
        metadata, links = render_page(
            source=Path(root, file),
            target=Path(target_dir, file),
        )
        PERMALINKS.append(metadata["permalink"])
        links_to_check[metadata["link"]] = links

render("sitemap.xml", "sitemap.xml", {"permalinks": PERMALINKS})
render("robots.txt", "robots.txt")

any_broken_links = False
for page, links in links_to_check.items():
    for link, anchor in links:
        # cv.pdf is generated and copied over in the deployment script
        if "://" in link or link in ["mailto:mike@barrucadu.co.uk", "cv.pdf"]:
            continue

        file_destination = Path(OUT_DIR, link)
        dir_destination = Path(OUT_DIR, link, "index.html")
        destination = None
        if file_destination.is_file():
            destination = file_destination
        elif dir_destination.is_file():
            destination = dir_destination
        else:
            print(f"BROKEN LINK {page}: {link}", file=sys.stderr)
            any_broken_links = True
            continue

        if anchor is not None:
            soup = BeautifulSoup(destination.read_text(), features="html.parser")
            if not soup.find(id=anchor):
                print(f"BROKEN LINK {page}: {link}#{anchor}", file=sys.stderr)
                any_broken_links = True

if any_broken_links:
    sys.exit(1)
