#!/usr/bin/env python3

"""build

Usage:
  build [--out=<dir>] [--root=<url>]
  build (-h | --help)

Options:
  -h --help       Show this screen.
  --out=<dir>     Directory to generate site in [default: _site]
  --root=<url>    Root of the website [default: https://www.barrucadu.co.uk/]

"""

import hashlib
import jinja2
import markdown
import re
import shutil
import sys
import xml.etree.ElementTree as ET
import yaml

from bs4 import BeautifulSoup
from docopt import docopt
from pathlib import Path
from slugify import slugify


class MdKonataProcessor(markdown.blockprocessors.BlockProcessor):
    RE_START = r"^<konata:([a-z-]+)>"
    RE_END = "</konata>$"

    RIGHT_KONATAS = ["flop", "ponder"]

    def test(self, parent, block):
        return re.search(self.RE_START, block)

    def run(self, parent, blocks):
        original_block = blocks[0]
        konata = re.search(self.RE_START, original_block).group(1)
        blocks[0] = re.sub(self.RE_START, "", original_block)
        for i, block in enumerate(blocks):
            if re.search(self.RE_END, block):
                blocks[i] = re.sub(self.RE_END, "", block)

                ebox = ET.SubElement(parent, "div")
                ebox.set("class", "box")
                eimgwr = ET.SubElement(ebox, "div")
                einner = ET.SubElement(ebox, "div")
                if konata in self.RIGHT_KONATAS:
                    einner, eimgwr = eimgwr, einner

                einner.set("class", "box-text")
                self.parser.parseBlocks(einner, blocks[0 : i + 1])

                eimg = ET.SubElement(eimgwr, "img")
                eimg.set("src", f"konata/{konata}.webp")
                eimg.set("alt", "")

                for i in range(0, i + 1):
                    blocks.pop(0)
                return True

        blocks[0] = original_block
        return False


class MdSectionPostprocessor(markdown.postprocessors.Postprocessor):
    def run(self, text):
        out = []
        in_section = False
        for line in text.splitlines():
            if line.startswith("<h2"):
                if in_section:
                    out.append("</section>")
                slug = slugify(line.split(">")[1].split("<")[0])
                out.append(f'<section id="{slug}">')
                in_section = True
            out.append(line)
        if in_section:
            out.append("</section>")
        return "\n".join(out)


class MdBarrucadu(markdown.extensions.Extension):
    def extendMarkdown(self, md):
        md.parser.blockprocessors.register(
            MdKonataProcessor(md.parser), "mdkonata", 999
        )
        md.postprocessors.register(MdSectionPostprocessor(), "mdsection", 999)


def filter_akk(body, jinja2_env):
    def parse_akkline(body, defns):
        out = {}
        while body:
            line = body[0]
            if line.startswith("%akk:trans"):
                body = body[1:]
                out["trans"] = line.split(maxsplit=1)[1].strip()
            elif line.startswith("%akk:orig"):
                out["orig"] = line.split(maxsplit=1)[1].strip()
                body = body[1:]
            elif line.startswith("%akk:note"):
                is_html = line.startswith("%akk:note:html")
                rest = line.split(maxsplit=1)[1].strip()
                out.setdefault("notes", []).append((rest, is_html))
                body = body[1:]
            elif line.startswith("%akk:vocab"):
                body = body[1:]
                rest = line.split(maxsplit=1)[1]
                if ":" in rest:
                    term, defn = rest.split(":", maxsplit=1)
                    term = term.strip()
                    defn = defn.strip()
                    defns[term.strip()] = defn
                else:
                    term = rest.strip()
                out.setdefault("vocab", []).append((term, defns[term]))
            else:
                break
        return out, body

    lineno = 0
    defns = {}
    fragment_template = jinja2_env.get_template("_fragments/akkline.html")

    while body:
        if body[0].startswith("%akk:trans"):
            akkline, body = parse_akkline(body, defns)
            if akkline:
                lineno += 1
                yield fragment_template.render(
                    **akkline,
                    lineno=lineno,
                )
        else:
            yield body[0]
            body = body[1:]


FILTERS = {
    "akk": filter_akk,
}


args = docopt(__doc__)
OUT_DIR = args["--out"]
BASE_HREF = args["--root"]

JINJA2_ENV = jinja2.Environment(
    autoescape=jinja2.select_autoescape(["html", "xml"]),
    loader=jinja2.FileSystemLoader("_templates"),
)

shutil.rmtree(OUT_DIR, ignore_errors=True)
shutil.copytree("static", OUT_DIR, dirs_exist_ok=True)

HASHED_LINKS = {}
for fpath in Path("hashed-static").iterdir():
    if not fpath.is_file():
        continue

    file_hash = hashlib.sha256()
    with fpath.open(mode="rb") as f:
        while True:
            data = f.read(65536)
            if not data:
                break
            file_hash.update(data)

    HASHED_LINKS[fpath.name] = (
        f"{fpath.stem}-sha256-{file_hash.hexdigest()}{fpath.suffix}"
    )
    shutil.copy(fpath, Path(OUT_DIR, HASHED_LINKS[fpath.name]))


def render(template, target, params={}):
    if type(template) is str:
        template = JINJA2_ENV.get_template(template)
    if type(target) is str:
        target = Path(OUT_DIR, target)

    params["link"] = str(target).replace(f"{OUT_DIR}/", "")
    params["permalink"] = f"{BASE_HREF}{params['link']}"

    rendered = template.render(
        base_href=BASE_HREF,
        hashed_links=HASHED_LINKS,
        site_name="barrucadu",
        **params,
    )
    target.write_text(rendered)

    return rendered


def render_page(source, target):
    lines = source.read_text().splitlines()
    metadata_end_idx = lines[1:].index("---") + 1
    body_start_idx = metadata_end_idx + 1
    metadata = yaml.load("\n".join(lines[1:metadata_end_idx]), Loader=yaml.SafeLoader)
    body = lines[body_start_idx:]

    metadata.setdefault("template", "default.html")
    metadata.setdefault("filters", [])

    if "feed" in metadata:
        for attr in ["published", "summary"]:
            if attr not in metadata["feed"]:
                print(f"MISSING FEED ATTRIBUTE {source}: {attr}", file=sys.stderr)
                sys.exit(1)
        metadata["feed"].setdefault("updated", metadata["feed"]["published"])
        metadata["feed"]["published"] = metadata["feed"]["published"].isoformat()
        metadata["feed"]["updated"] = metadata["feed"]["updated"].isoformat()

    for name in metadata["filters"]:
        body = list(FILTERS[name](body, JINJA2_ENV))

    if source.suffix == ".html":
        body = "\n".join(body)
    elif source.suffix == ".markdown":
        html = markdown.markdown(
            "\n".join(body),
            extensions=["fenced_code", "md_in_html", MdBarrucadu()],
        )
        body = f"{{% block content %}}\n{html}\n{{% endblock %}}"
    else:
        print(f"UNKNOWN FILETYPE {source}: {source.suffix}", file=sys.stderr)
        sys.exit(1)

    rendered = render(
        JINJA2_ENV.from_string(
            f"{{% extends \"{metadata['template']}\" %}}\n" + body,
        ),
        target,
        metadata,
    )

    soup = BeautifulSoup(rendered, features="html.parser")
    links = []
    for tag, attr in [("a", "href"), ("img", "src"), ("link", "href")]:
        for elem in soup.find_all(tag):
            link = elem.attrs[attr]
            anchor = None
            if "#" in link:
                link, anchor = link.split("#")
            links.append((link, anchor))
    if "feed" in metadata:
        if "image" in metadata["feed"]:
            links.append((metadata["feed"]["image"], None))

    return metadata, set(links)


PERMALINKS = []
FEED_ENTRIES = []
links_to_check = {}
for root, dirs, files in Path("content").walk():
    target_dir = Path(OUT_DIR, *root.parts[1:])
    for dir in dirs:
        Path(target_dir, dir).mkdir(exist_ok=True)
    for file in files:
        metadata, links = render_page(
            source=Path(root, file),
            target=Path(target_dir, file).with_suffix(".html"),
        )
        PERMALINKS.append(metadata["permalink"])
        if "feed" in metadata:
            FEED_ENTRIES.append(metadata)
        links_to_check[metadata["link"]] = links

FEED_ENTRIES = sorted(FEED_ENTRIES, key=lambda m: m["feed"]["updated"], reverse=True)
render(
    "atom.xml",
    "atom.xml",
    {
        "entries": FEED_ENTRIES[:10],
        "updated": FEED_ENTRIES[0]["feed"]["updated"],
    },
)
render("sitemap.xml", "sitemap.xml", {"permalinks": PERMALINKS})
render("robots.txt", "robots.txt")

any_broken_links = False
for page, links in links_to_check.items():
    for link, anchor in links:
        # cv.pdf is generated and copied over in the deployment script
        if "://" in link or link in ["mailto:mike@barrucadu.co.uk", "cv.pdf"]:
            continue

        file_destination = Path(OUT_DIR, link)
        dir_destination = Path(OUT_DIR, link, "index.html")
        destination = None
        if file_destination.is_file():
            destination = file_destination
        elif dir_destination.is_file():
            destination = dir_destination
        else:
            print(f"BROKEN LINK {page}: {link}", file=sys.stderr)
            any_broken_links = True
            continue

        if anchor is not None:
            soup = BeautifulSoup(destination.read_text(), features="html.parser")
            if not soup.find(id=anchor):
                print(f"BROKEN LINK {page}: {link}#{anchor}", file=sys.stderr)
                any_broken_links = True

if any_broken_links:
    sys.exit(1)
